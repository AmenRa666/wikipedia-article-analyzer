<mediawiki xmlns="http://www.mediawiki.org/xml/export-0.10/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.mediawiki.org/xml/export-0.10/ http://www.mediawiki.org/xml/export-0.10.xsd" version="0.10" xml:lang="en">
  <siteinfo>
    <sitename>Wikipedia</sitename>
    <dbname>enwiki</dbname>
    <base>https://en.wikipedia.org/wiki/Main_Page</base>
    <generator>MediaWiki 1.28.0-wmf.22</generator>
    <case>first-letter</case>
    <namespaces>
      <namespace key="-2" case="first-letter">Media</namespace>
      <namespace key="-1" case="first-letter">Special</namespace>
      <namespace key="0" case="first-letter" />
      <namespace key="1" case="first-letter">Talk</namespace>
      <namespace key="2" case="first-letter">User</namespace>
      <namespace key="3" case="first-letter">User talk</namespace>
      <namespace key="4" case="first-letter">Wikipedia</namespace>
      <namespace key="5" case="first-letter">Wikipedia talk</namespace>
      <namespace key="6" case="first-letter">File</namespace>
      <namespace key="7" case="first-letter">File talk</namespace>
      <namespace key="8" case="first-letter">MediaWiki</namespace>
      <namespace key="9" case="first-letter">MediaWiki talk</namespace>
      <namespace key="10" case="first-letter">Template</namespace>
      <namespace key="11" case="first-letter">Template talk</namespace>
      <namespace key="12" case="first-letter">Help</namespace>
      <namespace key="13" case="first-letter">Help talk</namespace>
      <namespace key="14" case="first-letter">Category</namespace>
      <namespace key="15" case="first-letter">Category talk</namespace>
      <namespace key="100" case="first-letter">Portal</namespace>
      <namespace key="101" case="first-letter">Portal talk</namespace>
      <namespace key="108" case="first-letter">Book</namespace>
      <namespace key="109" case="first-letter">Book talk</namespace>
      <namespace key="118" case="first-letter">Draft</namespace>
      <namespace key="119" case="first-letter">Draft talk</namespace>
      <namespace key="446" case="first-letter">Education Program</namespace>
      <namespace key="447" case="first-letter">Education Program talk</namespace>
      <namespace key="710" case="first-letter">TimedText</namespace>
      <namespace key="711" case="first-letter">TimedText talk</namespace>
      <namespace key="828" case="first-letter">Module</namespace>
      <namespace key="829" case="first-letter">Module talk</namespace>
      <namespace key="2300" case="first-letter">Gadget</namespace>
      <namespace key="2301" case="first-letter">Gadget talk</namespace>
      <namespace key="2302" case="case-sensitive">Gadget definition</namespace>
      <namespace key="2303" case="case-sensitive">Gadget definition talk</namespace>
      <namespace key="2600" case="first-letter">Topic</namespace>
    </namespaces>
  </siteinfo>
  <page>
    <title>Low Rank Matrix Approximations</title>
    <ns>0</ns>
    <id>48832912</id>
    <revision>
      <id>733604076</id>
      <parentid>725739978</parentid>
      <timestamp>2016-08-08T22:45:20Z</timestamp>
      <contributor>
        <username>FrescoBot</username>
        <id>9021902</id>
      </contributor>
      <minor/>
      <comment>Bot: [[User:FrescoBot/Links|link syntax]] and minor changes</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="15063">{{Orphan|date=January 2016}}

'''Low Rank Matrix Approximations''' are essential tools in the application of ''kernel methods to large-scale learning'' problems.&lt;ref&gt;Francis R. Bach and Michael I. Jordan (2005). [http://www.di.ens.fr/~fbach/bach_jordan_csi.pdf &quot;Predictive low-rank decomposition for kernel methods&quot;]. ''ICML''.&lt;/ref&gt;

Kernel methods (for instance, [[support vector machine]]s or [[gaussian process]]es&lt;ref name=&quot;:2&quot;&gt;{{cite journal |authors = Williams, C.K.I. and Seeger, M.|title = Using the Nyström method to speed up kernel machines|journal = Advances in Neural Information Processing Systems|year = 2001|url = http://papers.nips.cc/paper/1866-using-the-nystrom-method-to-speed-up-kernel-machines}}&lt;/ref&gt;) project data points into a high-dimensional or infinite-dimensional [[Feature vector|feature space]] and find the optimal splitting hyperplane. In the [[kernel method]] the data is represented in a ''kernel matrix'' (or, [[Gramian matrix|Gram matrix]]). Lots of algorithms can solve [[machine learning]] problems using the ''kernel matrix''. The main problem of [[kernel method]] is its high [[Algorithmic efficiency|computational cost]] associated with ''kernel matrices''. The cost is at least quadratic in the number of training data points, but most [[kernel method]]s include computation of [[Invertible matrix|matrix inversion]] or [[Eigendecomposition of a matrix|eigenvalue decomposition]] and the cost becomes cubic in the number of training data. Large training sets cause large [[Algorithmic efficiency|storage and computational costs]]. In spite of low rank decomposition methods ([[Cholesky decomposition]]) reduce this cost, they continue to require computing the ''kernel matrix''.  One of the approaches to deal with this problem is '''Low Rank Matrix Approximations'''. The most popular examples of them are [[Nyström method]] and the ''Random Features.'' Both of them have been successfully applied to efficient kernel learning.

== Nyström approximation ==
[[Kernel method]]s become unfeasible when the number of points &lt;math&gt;n&lt;/math&gt; is such that the kernel matrix &lt;math&gt;\hat{K}&lt;/math&gt; cannot be stored in memory.

If &lt;math&gt;n&lt;/math&gt; is the number of training examples, the [[Algorithmic efficiency|storage and computational cost]] required to find the solution of the problem using general [[kernel method]] is &lt;math&gt;O(n^2)&lt;/math&gt; and &lt;math&gt;O(n^3)&lt;/math&gt; respectively. The Nyström approximation can allow a significant speed-up of the computations.&lt;ref name=&quot;:2&quot; /&gt;&lt;ref name=&quot;:4&quot;&gt;Petros Drineas and Michael W. Mahoney (2005). [http://www.cs.yale.edu/homes/mmahoney/pubs/kernel_JMLR.pdf &quot;On the Nyström Method for Approximating a Gram Matrix for Improved Kernel-Based Learning&quot;.] ''Journal of Machine Learning Research 6'', pp. 2153–2175.&lt;/ref&gt; This speed-up is achieved by using instead of the ''kernel matrix'' its approximation &lt;math&gt;\tilde{K}&lt;/math&gt; of [[Rank (linear algebra)|rank]] &lt;math&gt;q&lt;/math&gt;. An advantage of the method is that it is not necessary to compute or store the whole ''kernel matrix'', but only its block of size &lt;math&gt;q \times n&lt;/math&gt;.

It reduces the storage and complexity requirements to &lt;math&gt; O(nq)&lt;/math&gt; and &lt;math&gt; O(nq^2)&lt;/math&gt; respectively.

=== Theorem for kernel approximation ===
&lt;math&gt;\hat{K}&lt;/math&gt; is a ''kernel matrix'' for some [[kernel method]]. Consider the first &lt;math&gt; q &lt; n&lt;/math&gt; points in the training set. Then there exists matrix &lt;math&gt;\tilde{K}&lt;/math&gt; of [[Rank (linear algebra)|rank]] &lt;math&gt;q&lt;/math&gt;:

&lt;math&gt;\tilde{K}=\hat{K}_{n,q}\hat{K}_{q}^{-1}\hat{K}_{n,q}^\text{T}&lt;/math&gt; , where

&lt;math&gt;(\hat{K}_q)_{i,j} = K(x_i,x_j), i, j = 1,\dots,q&lt;/math&gt; ,

&lt;math&gt;\hat{K}_q&lt;/math&gt; is invertible matrix

and

&lt;math&gt;(\hat{K}_{n,q})_{i,j} = K(x_i,x_j), i = 1,\dots, n \text{ and } j = 1,\dots,q.&lt;/math&gt;

==== &lt;u&gt;Proof&lt;/u&gt; ====

===== Singular Value Decomposition application =====
Applying [[Singular value decomposition|Singular Value Decomposition (SVD)]] &lt;nowiki/&gt;to matrix &lt;math&gt;
A&lt;/math&gt; with dimensions &lt;math&gt;
p \times m&lt;/math&gt; produces a ''singular system''  consisting of [[singular value]]s &lt;math&gt;
\{\sigma_j\}_{j=1}^k,\text{ } (\sigma_j &gt; 0 \text{ } \forall j=1,\dots,k),&lt;/math&gt;  vectors &lt;math&gt;
\{v_j\}_{j=1}^m \in \mathbb{C}^m&lt;/math&gt; and &lt;math&gt;
\{u_j\}_{j=1}^p \in \mathbb{C}^p&lt;/math&gt; such that they form orthonormal bases of &lt;math&gt;
\mathbb{C}^m&lt;/math&gt; and &lt;math&gt;
\mathbb{C}^p&lt;/math&gt; respectively:

&lt;math&gt;
\begin{cases} A^\text{T}Av_j = \sigma_j v_j, \text{ } j=1,\dots,k,\\ A^\text{T}Av_j =0, \text{ }  j=k+1,\dots,m,\\
AA^\text{T}u_j=\sigma_j u_j, \text{ } j=1,\dots,k,\\ AA^\text{T}u_j=0, \text{ }  j=k+1,\dots,p. \end{cases}&lt;/math&gt;

If &lt;math&gt;
U&lt;/math&gt; and &lt;math&gt;
V&lt;/math&gt; are matrices with &lt;math&gt;
u&lt;/math&gt;’s and &lt;math&gt;
v&lt;/math&gt;’s in the columns and &lt;math&gt;\Sigma&lt;/math&gt; is a [[Diagonal matrix|diagonal]] &lt;math&gt;p \times m&lt;/math&gt; matrix having [[singular value]]s &lt;math&gt;\sigma_i&lt;/math&gt; on the first &lt;math&gt;k&lt;/math&gt;-entries on the diagonal (all the other elements of the matrix are zeros):

&lt;math&gt;\begin{cases} 
Av_j = \sqrt{\sigma_j}u_j, \text{ } j=1,\dots,k, \\
Av_j=0, \text{ } j=k+1,\dots,m, \\
A^\text{T}u_j= \sqrt{\sigma_j}v_j, \text{ } j=1,\dots,k,\\
A^\text{T}u_j=0, \text{ } j=k+1,\dots,p,
\end{cases}&lt;/math&gt;

then the matrix &lt;math&gt;A&lt;/math&gt; can be rewritten as:&lt;ref name=&quot;:1&quot;&gt;Lorenzo Rosasco, Mikhail Belkin and Ernesto De Vito (2010). [http://web.cse.ohio-state.edu/~mbelkin/papers/LIO_JMLR_10.pdf &quot;On Learning with Integral Operators&quot;.] ''Journal of Machine Learning Research 11'', pp. 905-934.&lt;/ref&gt;

&lt;math&gt;A=U\Sigma^{1/2}V^\text{T}&lt;/math&gt;.

===== Further proof =====
* &lt;math&gt;\hat{X} &lt;/math&gt; is &lt;math&gt; n \times D &lt;/math&gt; data matrix
* &lt;math&gt;\hat{K}=\hat{X}\hat{X}^\text{T} &lt;/math&gt;
* &lt;math&gt;\hat{C}=\hat{X}^\text{T}\hat{X} &lt;/math&gt;

Applying [[Singular value decomposition|Singular Value Decomposition (SVD)]] to these matrices:

&lt;math&gt;\hat{X}=\hat{U}\hat{\Sigma}^{1/2}\hat{V}^\text{T},\text{ }\hat{K}=\hat{U}\hat{\Sigma}\hat{U}^{T}, 
\text{ } \hat{C}=\hat{V}\hat{\Sigma}\hat{V}^\text{T}. &lt;/math&gt;
* &lt;math&gt;\hat{X}_q&lt;/math&gt; is the &lt;math&gt;q\times D&lt;/math&gt;-dimensional matrix consisting of the first &lt;math&gt;q&lt;/math&gt; raws of matrix &lt;math&gt;\hat{X}&lt;/math&gt;
* &lt;math&gt;\hat{K}_q=\hat{X}_q\hat{X}_q^\text{T} &lt;/math&gt;
* &lt;math&gt;\hat{C}=\hat{X}^\text{T}\hat{X} &lt;/math&gt;
Applying [[Singular value decomposition|Singular Value Decomposition (SVD)]] to these matrices:

&lt;math&gt;\hat{X}_q=\hat{U}_q\hat{\Sigma}_q^{1/2}\hat{V}_q^\text{T},\text{ }\hat{K}_q=\hat{U}_q\hat{\Sigma}_q\hat{U}_q^{T}, 
\text{ } \hat{C}_q=\hat{V}_q\hat{\Sigma}_q\hat{V}_q^\text{T}. &lt;/math&gt;

Since &lt;math&gt;\hat{U},\text{ } \hat{V}, \hat{U}_q\text{ and } \hat{V}_q  &lt;/math&gt; are [[Orthogonal matrix|orthogonal matrices]],

&lt;math&gt;\hat{U}=\hat{X}\hat{V}\hat{\Sigma}^{-1/2}, \text{ }\hat{V}_q=\hat{X}_q^\text{T}\hat{U}_q\hat{\Sigma}_q^{-1/2}. &lt;/math&gt;

Replacing &lt;math&gt;\hat{V},\text{ } \hat{\Sigma} \text{ by } \hat{V}_q\text{ and } \hat{\Sigma}_q  &lt;/math&gt;,  an approximation for &lt;math&gt;\hat{U}  &lt;/math&gt; can be obtained:

&lt;math&gt;\tilde{U}=\hat{X}\hat{X}_q^\text{T}\hat{U}_q\hat{\Sigma}_q^{-1}  &lt;/math&gt; ( &lt;math&gt;\tilde{U}  &lt;/math&gt; is not necessarily an [[orthogonal matrix]]).

However, defining &lt;math&gt;\tilde{K}=\tilde{U}\hat{\Sigma}_q \tilde{U}^\text{T} &lt;/math&gt;, it can be computed the next way:

&lt;math&gt;\begin{align} 
\tilde{K}=\tilde{U}\hat{\Sigma}_q \tilde{U}^\text{T} =  \hat{X}\hat{X}_q^\text{T}\hat{U}_q\hat{\Sigma}_q^{-1} \hat{\Sigma}_q (\hat{X}\hat{X}_q^\text{T}\hat{U}_q \hat{\Sigma}_q^{-1} )^\text{T} \\
= \hat{X}\hat{X}_q^\text{T}\big\{\hat{U}_q (\hat{\Sigma}_q^{-1})^\text{T}\hat{U}_q^\text{T}\big\}(\hat{X}\hat{X}_q^\text{T})^\text{T} \\ 
\end{align}  &lt;/math&gt;

By the characterization for [[orthogonal matrix]] &lt;math&gt;\hat{U}_q  &lt;/math&gt; :  equality &lt;math&gt;(\hat{U}_q)^\text{T}=(\hat{U}_q)^{-1}  &lt;/math&gt; holds. Then, using the formula for the inverse of [[matrix multiplication]] &lt;math&gt;(AB)^{-1}=B^{-1}A^{-1}  &lt;/math&gt; for [[Invertible matrix|invertible matrices]] &lt;math&gt;A  &lt;/math&gt; and &lt;math&gt;B  &lt;/math&gt;, the expression in braces can be rewritten as:

&lt;math&gt;\begin{align} 
\hat{U}_q (\hat{\Sigma}_q^{-1})^\text{T}\hat{U}_q^\text{T} = \hat{U}_q \hat{\Sigma}_q^\text{T}\hat{U}_q^\text{T} = \hat{K}_q
\end{align}  &lt;/math&gt; .

Then the expression for &lt;math&gt;\tilde{K}  &lt;/math&gt;:

&lt;math&gt;\begin{align} 
\tilde{K}=(\hat{X}\hat{X}_q^\text{T})\hat{K}_q^{-1}(\hat{X}\hat{X}_q^\text{T})^\text{T} \\ 
\end{align}  &lt;/math&gt;.

Defining &lt;math&gt;\hat{K}_{n,q}=\hat{X}\hat{X}_q^\text{T}  &lt;/math&gt;, the proof is finished.

=== General theorem for kernel approximation for a feature map ===
For a feature map &lt;math&gt;\Phi: \mathcal{X} \rightarrow \mathcal{F}  &lt;/math&gt; with associated [[Kernel method|kernel]] &lt;math&gt;K(x,x') = \langle \Phi(x),\Phi(x')\rangle_\mathcal{F}   &lt;/math&gt;: equality &lt;math&gt;\hat{K}=\hat{K}_{n,q}\hat{K}_{q}^{-1}\hat{K}_{n,q}^\text{T}&lt;/math&gt; also follows by replacing &lt;math&gt;\hat{X}  &lt;/math&gt;  by the operator &lt;math&gt;\hat{\Phi}: \mathcal{F} \rightarrow \mathbb{R}^n  &lt;/math&gt; such that &lt;math&gt;\langle \hat{\Phi}w\rangle_i = \langle \hat{\Phi}(x_i), w\rangle_\mathcal{F}&lt;/math&gt;, &lt;math&gt;\text { } i=1,\dots,n&lt;/math&gt;,  &lt;math&gt;w \in \mathcal{F}&lt;/math&gt;, and &lt;math&gt;\hat{X}_q&lt;/math&gt;  by the operator &lt;math&gt;\hat{\Phi}_q: \mathcal{F} \rightarrow \mathbb{R}^q  &lt;/math&gt; such that &lt;math&gt;\langle \hat{\Phi}w\rangle_i = \langle \hat{\Phi}(x_i), w\rangle_\mathcal{F}&lt;/math&gt;, &lt;math&gt;\text { } i=1,\dots,q&lt;/math&gt;, &lt;math&gt;w \in \mathcal{F}&lt;/math&gt; . Once again, a simple inspection shows that the feature map is only needed in the proof while the end result only depends on computing the kernel function.

=== Application for Regularized Least Squares ===
In a vector and kernel notation, the problem of [[Regularized least squares|Regularized Least Squares]] can be rewritten as:
: &lt;math&gt;
\min_{c \in \mathbb{R}^{n}}\frac{1}{n}\|\hat{Y}-\hat{K}c\|^{2}_{\mathbb{R}^{n}} + \lambda\langle c,\hat{K}c\rangle_{\mathbb{R}^{n}} 
&lt;/math&gt;. 
Computing the gradient and setting in to 0, the minimum can be obtained:
: &lt;math&gt;
\begin{align} -\frac{1}{n}\hat{K}(\hat{Y}-\hat{K}c) + \lambda \hat{K}c = 0 \Rightarrow \hat{K}(\hat{K}+\lambda n I)c = \hat{K} \hat{Y} \\ 
 \Rightarrow c  = (\hat{K}+\lambda n I)^{-1}\hat{Y}, \text{ where } c \in \mathbb{R}^{n} \\ \end{align}&lt;/math&gt;
The inverse matrix &lt;math&gt;
(\hat{K}+\lambda n I)^{-1}&lt;/math&gt; can be computed using [[Woodbury matrix identity]]:

&lt;math&gt;
\begin{align} (\hat{K}+\lambda n I)^{-1} = \cfrac{1}{\lambda n}\bigg(\cfrac{1}{\lambda n}\hat{K} + I\bigg)^{-1}
\\=\cfrac{1}{\lambda n}\bigg(I + \hat{K}_{n,q}({\lambda n}\hat{K}_{q})^{-1}\hat{K}_{n,q}^\text{T}\bigg)^{-1} \\ = \cfrac{1}{\lambda n}\Big(I-\hat{K}_{n,q}(\lambda n\hat{K}_{q}+\hat{K}_{n,q}^\text{T} \hat{K}_{n,q})^{-1}\hat{K}_{n,q}^\text{T}\Big) \\ \end{align} &lt;/math&gt;

It has the desired storage and complexity requirements.

== Randomized feature maps approximation ==
Let &lt;math&gt; \mathbf{x}, \mathbf{x'} \in \mathbb{R}^d&lt;/math&gt; - samples of data, &lt;math&gt; z: \mathbb{R}^d \rightarrow \mathbb{R}^D&lt;/math&gt; - a randomized [[feature map]] (maps a single vector to a vector of higher dimensionality) so that the inner product between a pair of transformed points approximates their [[Kernel method|kernel]] evaluation:

&lt;math&gt; K(\mathbf{x}, \mathbf{x'}) = \langle\Phi(\mathbf{x}),\Phi(\mathbf{x'})\rangle \approx z(\mathbf{x})^\text{T}z(\mathbf{x'})&lt;/math&gt;,

where &lt;math&gt; \Phi&lt;/math&gt; is the mapping embedded in the [[Radial basis function kernel|RBF kernel]].

Since &lt;math&gt; z&lt;/math&gt; is low-dimensional, the input can be easily transformed with &lt;math&gt; z&lt;/math&gt;, after that different linear learning methods to approximate the answer of the corresponding nonlinear kernel can be applied. There are different randomized feature maps to compute the approximations to the [[Radial basis function kernel|RBF kernels]]. For instance, '''Random Fourier Features''' and '''Random Binning Features'''.

=== Random Fourier Features ===
'''Random Fourier Features''' map produces a [[Monte Carlo method|Monte-Carlo]] approximation to the feature map. [[Monte Carlo method|Monte-Carlo method]] is considered to be randomized. These ''random features'' consists of sinusoids &lt;math&gt;\cos(w^\text{T}\mathbf{x}+b)&lt;/math&gt; randomly drawn from [[Fourier transform]] of the [[Kernel method|kernel]] to be approximated, where &lt;math&gt;w \in \mathbb{R}^d&lt;/math&gt; and &lt;math&gt;b \in \mathbb{R}&lt;/math&gt; are [[random variable]]s. The line is randomly chosen, then the data points are projected on it by the mappings. The resulting scalar is passed through a sinusoid.  The product of the transformed points will approximate a shift-invariant [[Kernel method|kernel]]. Since the map is smooth, '''Random Fourier Features''' work well on interpolation tasks.

=== Random Binning Features ===
'''Random Binning Features''' map partitions the input space using randomly shifted grids at randomly chosen resolutions and assigns to an input point a binary bit string that corresponds to the bins in which it falls. The grids are constructed so that the probability that two points &lt;math&gt; \mathbf{x}, \mathbf{x'} \in \mathbb{R}^d&lt;/math&gt; are assigned to the same bin is proportional to &lt;math&gt; K(\mathbf{x}, \mathbf{x'})&lt;/math&gt;. The inner product between a pair of transformed points is proportional to the number of times the two points are binned together, and is therefore an unbiased estimate of &lt;math&gt; K(\mathbf{x}, \mathbf{x'})&lt;/math&gt;. Since this mapping is not smooth and uses the [[distance|proximity]] between input points, Random Binning Features works well for approximating kernels that depend only on the [[L1 distance|&lt;math&gt;L_1&lt;/math&gt; - distance]] between datapoints.

== Comparison of approximation methods ==
The approaches for large-scale kernel learning ([[Nyström method]] and '''Random Features''') differs in the fact that the [[Nyström method]] uses data dependent basis functions while in '''Random Features''' approach the basis functions are sampled from a distribution independent from the training data. This difference leads to an improved analysis for kernel learning approaches based on the [[Nyström method]]. When there is a large gap in the eigen-spectrum of the [[Kernel method|kernel]] matrix, approaches based on the [[Nyström method]] can achieve better results than '''Random Features''' based approach.&lt;ref name=&quot;:3&quot;&gt;Tianbao Yang, Yu-Feng Li, Mehrdad Mahdavi, Rong Jin and Zhi-Hua Zhou  (2012). [http://papers.nips.cc/paper/4588-nystrom-method-vs-random-fourier-features-a-theoretical-and-empirical-comparison.pdf &quot;Nyström Method vs Random Fourier Features: A Theoretical and Empirical Comparison&quot;.] ''Advances in Neural Information Processing Systems 25 (NIPS)''.&lt;/ref&gt;

==See also==
* [[Nyström method]]
* [[Support vector machine|Support Vector Machine]]
* [[Radial basis function kernel]]
* [[Regularized least squares|Regularized Least Squares]]

== External links ==
* Andreas Müller (2012). [http://peekaboo-vision.blogspot.de/2012/12/kernel-approximations-for-efficient.html Kernel Approximations for Efficient SVMs (and other feature extraction methods)].

==References==
{{reflist|30em}}

[[Category:Kernel methods for machine learning]]</text>
      <sha1>thgbq8x38pnvfnncu4vir8sl19h4iaj</sha1>
    </revision>
  </page>
</mediawiki>
